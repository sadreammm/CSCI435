{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea705bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bde46",
   "metadata": {},
   "source": [
    "# **Landmark Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a8056e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create()\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b52c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_dir = 'content/museum_images'\n",
    "\n",
    "landmark_descriptors = []\n",
    "landmark_keypoints = []\n",
    "landmark_images = []\n",
    "\n",
    "for img_name in os.listdir(landmark_dir):\n",
    "    img_path = os.path.join(landmark_dir, img_name)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        continue\n",
    "    img = cv2.resize(img, (600, 400))\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    if des is not None:\n",
    "        landmark_keypoints.append(kp)\n",
    "        landmark_descriptors.append(des)\n",
    "        landmark_images.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec520c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmark(query_img, match_threshold=10):\n",
    "    if query_img is None:\n",
    "        return query_img\n",
    "    query_img = cv2.resize(query_img, (600, 400))\n",
    "    query_kp, query_des = sift.detectAndCompute(query_img, None)\n",
    "\n",
    "    best_match_count = 0\n",
    "    best_good_matches = []\n",
    "    best_index = -1\n",
    "    best_homography = None\n",
    "    best_pts = None\n",
    "\n",
    "    for i, des in enumerate(landmark_descriptors):\n",
    "        matches = bf.match(des, query_des)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "        good_matches = matches[:50]\n",
    "\n",
    "        if len(good_matches) >= match_threshold:\n",
    "            src_pts = np.float32([landmark_keypoints[i][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "            dst_pts = np.float32([query_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "            H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            matches_mask = mask.ravel().tolist()\n",
    "            inliers = sum(matches_mask)\n",
    "\n",
    "            if H is not None and inliers > best_match_count:\n",
    "                best_match_count = inliers\n",
    "                best_good_matches = good_matches\n",
    "                best_index = i\n",
    "                best_homography = H\n",
    "\n",
    "    if best_match_count >= match_threshold:\n",
    "        print(f\"✅ Landmark detected with {best_match_count} inlier matches.\")\n",
    "\n",
    "        matched_img = cv2.drawMatches(\n",
    "            landmark_images[best_index], landmark_keypoints[best_index],\n",
    "            query_img, query_kp,\n",
    "            best_good_matches, None,\n",
    "            matchColor=(0, 255, 0),\n",
    "            singlePointColor=None,\n",
    "            matchesMask=[1]*len(best_good_matches),\n",
    "            flags=2\n",
    "        )\n",
    "\n",
    "        return matched_img\n",
    "    else:\n",
    "        return query_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7397789",
   "metadata": {},
   "source": [
    "Landmark Detection on Still Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430805a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Landmark detected with 32 inlier matches.\n"
     ]
    }
   ],
   "source": [
    "query_img = cv2.imread('content/query.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "matched_img = detect_landmark(query_img)\n",
    "cv2.imshow('Matched Image', matched_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb89eb56",
   "metadata": {},
   "source": [
    "Real-Time Landmark Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2270694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Landmark detected with 15 inlier matches.\n",
      "✅ Landmark detected with 46 inlier matches.\n",
      "✅ Landmark detected with 44 inlier matches.\n",
      "✅ Landmark detected with 48 inlier matches.\n",
      "✅ Landmark detected with 45 inlier matches.\n",
      "✅ Landmark detected with 43 inlier matches.\n",
      "✅ Landmark detected with 49 inlier matches.\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    result = detect_landmark(frame, match_threshold=10)\n",
    "    cv2.imshow(\"Landmark Detection\", result)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b9408",
   "metadata": {},
   "source": [
    "# **Face Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af983e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f9ea7",
   "metadata": {},
   "source": [
    "Create Encoding of Known Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c04b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "path = \"known_faces\"\n",
    "for filename in os.listdir(path):\n",
    "    img_path = os.path.join(path, filename)\n",
    "    name = os.path.splitext(filename)[0]\n",
    "    img = face_recognition.load_image_file(img_path)\n",
    "    encoding = face_recognition.face_encodings(img)\n",
    "    if encoding:\n",
    "        known_face_encodings.append(encoding[0])\n",
    "        known_face_names.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefee68e",
   "metadata": {},
   "source": [
    "Haar Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2ef525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(img):\n",
    "    face_img = img.copy()\n",
    "    face_rects = face_cascade.detectMultiScale(face_img, scaleFactor=1.2, minNeighbors=5)\n",
    "    return face_rects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b67300",
   "metadata": {},
   "source": [
    "face_recognition Library for Recognizing New Faces by Comparing with Known Faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83640e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces(img, face_rects):\n",
    "    face_img = img.copy()\n",
    "    for (x, y, w, h) in face_rects:\n",
    "        roi = face_img[y:y+h, x:x+w]\n",
    "        rgb_face = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "        encodings = face_recognition.face_encodings(rgb_face)\n",
    "        name = \"Unknown\"\n",
    "        if encodings:\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, encodings[0])\n",
    "            face_distances = face_recognition.face_distance(known_face_encodings, encodings[0])\n",
    "\n",
    "            if True in matches:\n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                name = known_face_names[best_match_index]\n",
    "        \n",
    "        cv2.rectangle(face_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        cv2.putText(face_img, name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e167f223",
   "metadata": {},
   "source": [
    "Face Recognition on Still Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "693084ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = cv2.imread('content/query_elon.jpg')\n",
    "face_rects = detect_faces(query_img)\n",
    "if len(face_rects) > 0:\n",
    "    recognized_img = recognize_faces(query_img, face_rects)\n",
    "    cv2.imshow('Recognized Faces', recognized_img)\n",
    "    cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6ce39",
   "metadata": {},
   "source": [
    "Real-Time Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    face_rects = detect_faces(frame)\n",
    "    result = recognize_faces(frame, face_rects)\n",
    "    cv2.imshow(\"Face Recognition\", result)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3114dfd",
   "metadata": {},
   "source": [
    "**Real-Time Landmark and Face Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cba07776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Landmark detected with 12 inlier matches.\n",
      "✅ Landmark detected with 14 inlier matches.\n",
      "✅ Landmark detected with 13 inlier matches.\n",
      "✅ Landmark detected with 10 inlier matches.\n",
      "✅ Landmark detected with 15 inlier matches.\n",
      "✅ Landmark detected with 15 inlier matches.\n",
      "✅ Landmark detected with 12 inlier matches.\n",
      "✅ Landmark detected with 19 inlier matches.\n",
      "✅ Landmark detected with 12 inlier matches.\n",
      "✅ Landmark detected with 11 inlier matches.\n"
     ]
    }
   ],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    face_rects = detect_faces(frame)\n",
    "    frame = recognize_faces(frame, face_rects)\n",
    "    result = detect_landmark(frame, match_threshold=10)\n",
    "    cv2.imshow(\"Landmark Detection\", result)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef25de8",
   "metadata": {},
   "source": [
    "# **Time of Day Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c1e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_day_path = \"Images\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee3050",
   "metadata": {},
   "source": [
    "Quantitative Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334dda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_brightness_contrast_metrics(img):\n",
    "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    mean_rgb = np.mean(rgb)\n",
    "    contrast_rgb = np.std(rgb)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    mean_hsv_v = np.mean(hsv[:,:,2])\n",
    "    contrast_hsv_v = np.std(hsv[:,:,2])\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    mean_lab_l = np.mean(lab[:,:,0])\n",
    "    contrast_lab_l = np.std(lab[:,:,0])\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    mean_gray = np.mean(gray)\n",
    "    contrast_gray = np.std(gray)\n",
    "    return {\n",
    "        \"mean_rgb\": mean_rgb, \"contrast_rgb\": contrast_rgb,\n",
    "        \"mean_hsv_v\": mean_hsv_v, \"contrast_hsv_v\": contrast_hsv_v,\n",
    "        \"mean_lab_l\": mean_lab_l, \"contrast_lab_l\": contrast_lab_l,\n",
    "        \"mean_gray\": mean_gray, \"contrast_gray\": contrast_gray\n",
    "    }\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "for filename in os.listdir(time_of_day_path):\n",
    "    img_path = os.path.join(time_of_day_path, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "    metrics = compute_brightness_contrast_metrics(img)\n",
    "    metrics[\"filename\"] = filename\n",
    "    results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1004802b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename                  RGB    C_RGB    HSV_V    C_HSV    LAB_L    C_LAB     GRAY   C_GRAY\n",
      "Image_1.jpg            139.26    59.88   163.17    62.14   147.26    50.40   138.39    50.16\n",
      "Image_10.jpg            59.60    47.00    73.82    50.55    61.85    48.18    59.34    44.69\n",
      "Image_11.jpg            66.06    47.73    86.26    57.39    72.78    50.15    69.00    45.41\n",
      "Image_12.jpg            50.88    45.92    61.78    52.05    51.51    47.54    50.44    44.03\n",
      "Image_13.jpg            52.01    51.04    66.74    59.61    56.07    55.33    54.06    50.72\n",
      "Image_14.jpg            49.16    37.21    64.56    42.44    49.37    37.87    47.99    34.28\n",
      "Image_15.jpg            45.30    45.51    69.22    51.08    46.86    44.42    44.06    40.04\n",
      "Image_16.jpg            31.28    30.99    34.21    32.77    29.96    33.41    31.44    30.95\n",
      "Image_2.jpg            157.40    70.69   171.66    70.19   160.98    69.32   155.42    68.53\n",
      "Image_3.jpeg           181.98    58.56   197.74    51.69   185.65    53.28   179.56    56.20\n",
      "Image_4.JPEG           142.00    62.58   162.53    57.26   154.46    54.02   144.90    55.28\n",
      "Image_5.jpg            136.33    60.37   149.21    58.94   143.86    57.44   136.72    57.66\n",
      "Image_6.jpg            172.28    70.53   185.00    62.70   181.23    65.42   174.59    68.10\n",
      "Image_7.jpg            123.45    65.98   150.95    70.69   129.05    60.76   119.73    57.93\n",
      "Image_8.jpg            132.18    67.03   162.90    67.38   136.81    58.54   128.48    57.73\n",
      "Image_9.jpg             74.66    47.18    86.61    49.82    80.33    49.99    76.20    46.82\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Filename':<20} {'RGB':>8} {'C_RGB':>8} {'HSV_V':>8} {'C_HSV':>8} {'LAB_L':>8} {'C_LAB':>8} {'GRAY':>8} {'C_GRAY':>8}\")\n",
    "for r in results:\n",
    "    print(f\"{r['filename']:<20} {r['mean_rgb']:8.2f} {r['contrast_rgb']:8.2f} {r['mean_hsv_v']:8.2f} {r['contrast_hsv_v']:8.2f} {r['mean_lab_l']:8.2f} {r['contrast_lab_l']:8.2f} {r['mean_gray']:8.2f} {r['contrast_gray']:8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1756b8",
   "metadata": {},
   "source": [
    "Classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e08b94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {\n",
    "    \"mean_hsv_v\": 100,\n",
    "    \"mean_lab_l\": 110,\n",
    "    \"mean_rgb\": 120,\n",
    "    \"mean_gray\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f0ab329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_display(img, filename, metric=\"mean_hsv_v\"):\n",
    "    metrics = compute_brightness_contrast_metrics(img)\n",
    "    value = metrics[metric]\n",
    "    threshold = thresholds[metric]\n",
    "    label = \"Day\" if value > threshold else \"Night\"\n",
    "    print(f\"\\n{filename} - {metric}: {value:.2f} - Prediction: {label}\")\n",
    "    display_img = img.copy()\n",
    "    cv2.putText(display_img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv2.LINE_AA)\n",
    "    cv2.imshow(f\"{filename} - {label}\", display_img)\n",
    "    print(\"Press any key in the image window to close it.\")\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "718a4101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available metrics for classification:\n",
      "1. mean_hsv_v (recommended)\n",
      "2. mean_lab_l\n",
      "3. mean_rgb\n",
      "4. mean_gray\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAvailable metrics for classification:\")\n",
    "print(\"1. mean_hsv_v (recommended)\")\n",
    "print(\"2. mean_lab_l\")\n",
    "print(\"3. mean_rgb\")\n",
    "print(\"4. mean_gray\")\n",
    "metric_map = {\n",
    "    \"1\": \"mean_hsv_v\",\n",
    "    \"2\": \"mean_lab_l\",\n",
    "    \"3\": \"mean_rgb\",\n",
    "    \"4\": \"mean_gray\"\n",
    "}\n",
    "user_metric = input(\"Choose metric (1-4): \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "863bf618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image_1.jpg - mean_hsv_v: 163.17 - Prediction: Day\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_10.jpg - mean_hsv_v: 73.82 - Prediction: Night\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_11.jpg - mean_hsv_v: 86.26 - Prediction: Night\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_12.jpg - mean_hsv_v: 61.78 - Prediction: Night\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_13.jpg - mean_hsv_v: 66.74 - Prediction: Night\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_14.jpg - mean_hsv_v: 64.56 - Prediction: Night\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_15.jpg - mean_hsv_v: 69.22 - Prediction: Night\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_16.jpg - mean_hsv_v: 34.21 - Prediction: Night\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_2.jpg - mean_hsv_v: 171.66 - Prediction: Day\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_3.jpeg - mean_hsv_v: 197.74 - Prediction: Day\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_4.JPEG - mean_hsv_v: 162.53 - Prediction: Day\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_5.jpg - mean_hsv_v: 149.21 - Prediction: Day\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_6.jpg - mean_hsv_v: 185.00 - Prediction: Day\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_7.jpg - mean_hsv_v: 150.95 - Prediction: Day\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_8.jpg - mean_hsv_v: 162.90 - Prediction: Day\n",
      "Press any key in the image window to close it.\n",
      "\n",
      "Image_9.jpg - mean_hsv_v: 86.61 - Prediction: Night\n",
      "Press any key in the image window to close it.\n"
     ]
    }
   ],
   "source": [
    "metric = metric_map.get(user_metric, \"mean_hsv_v\")\n",
    "\n",
    "for filename in os.listdir(time_of_day_path):\n",
    "    img_path = os.path.join(time_of_day_path, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "    classify_and_display(img, filename, metric)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc4a3f",
   "metadata": {},
   "source": [
    "# **Image Quality Assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e3d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"IQA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ce2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_text(text, font, font_scale, max_width):\n",
    "    words = text.split(' ')\n",
    "    lines = []\n",
    "    current_line = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        test_line = current_line + (\" \" if current_line else \"\") + word\n",
    "        text_size = cv2.getTextSize(test_line, font, font_scale, 1)[0]\n",
    "        \n",
    "        if text_size[0] <= max_width:\n",
    "            current_line = test_line\n",
    "        else:\n",
    "            if current_line:\n",
    "                lines.append(current_line)\n",
    "                current_line = word\n",
    "            else:\n",
    "                lines.append(word)\n",
    "    \n",
    "    if current_line:\n",
    "        lines.append(current_line)\n",
    "    \n",
    "    return lines\n",
    "\n",
    "def draw_wrapped_text(img, text, start_pos, font, font_scale, color, thickness, max_width):\n",
    "    lines = wrap_text(text, font, font_scale, max_width)\n",
    "    x, y = start_pos\n",
    "    line_height = cv2.getTextSize(\"Ay\", font, font_scale, thickness)[0][1] + 10\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        cv2.putText(img, line, (x, y + i * line_height), font, font_scale, color, thickness)\n",
    "    \n",
    "    return y + len(lines) * line_height"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c74f46f",
   "metadata": {},
   "source": [
    "Detect Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d908f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_blur(img):\n",
    "    threshold=100\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    variance = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    return variance < threshold, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4dd9a",
   "metadata": {},
   "source": [
    "Highlight Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a5feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_blur_regions(img, gray, threshold=100):\n",
    "    height, width = gray.shape\n",
    "    window_size = min(height, width) // 8\n",
    "    if window_size < 50:\n",
    "        window_size = 50\n",
    "    \n",
    "    overlay = img.copy()\n",
    "    \n",
    "    for y in range(0, height - window_size, window_size // 2):\n",
    "        for x in range(0, width - window_size, window_size // 2):\n",
    "            window = gray[y:y+window_size, x:x+window_size]\n",
    "            laplacian_var = cv2.Laplacian(window, cv2.CV_64F).var()\n",
    "            \n",
    "            if laplacian_var < threshold:\n",
    "                cv2.rectangle(overlay, (x, y), (x+window_size, y+window_size), (0, 0, 255), -1)\n",
    "    return cv2.addWeighted(img, 0.8, overlay, 0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2235d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_blur_path = \"IQA/detect_blur\"\n",
    "for filename in os.listdir(detect_blur_path):\n",
    "    img_path = os.path.join(detect_blur_path, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    height, width = img.shape[:2]\n",
    "    max_text_width = width - 20\n",
    "    \n",
    "    text = \"Not Blurry\"\n",
    "    blurred, variance = is_blur(img)\n",
    "    \n",
    "    if blurred:\n",
    "        text = \"Blurry\"\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = highlight_blur_regions(img, gray)\n",
    "\n",
    "    cv2.putText(img, f\"{text}: {variance:.2f}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    \n",
    "    if blurred:\n",
    "        enhancement_text = \"Enhancement: Apply sharpening filter (Laplacian) or recapture with stable camera.\"\n",
    "        draw_wrapped_text(img, enhancement_text, (10, 70), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, max_text_width)\n",
    "    \n",
    "    cv2.imshow(\"Image Quality Assessment - Blur\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a24949f",
   "metadata": {},
   "source": [
    "Detect Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb2800e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_exposure(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "    total_pixels = gray.size\n",
    "    dark_pixels = np.sum(hist[:50])\n",
    "    bright_pixels = np.sum(hist[205:])\n",
    "\n",
    "    dark_ratio = dark_pixels / total_pixels\n",
    "    bright_ratio = bright_pixels / total_pixels\n",
    "\n",
    "    if dark_ratio > 0.3:\n",
    "        return \"Underexposed\"\n",
    "    elif bright_ratio > 0.3:\n",
    "        return \"Overexposed\"\n",
    "    else:\n",
    "        return \"Well-exposed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5c83f2",
   "metadata": {},
   "source": [
    "Highlight Exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "277ade1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_exposure_regions(img, exposure_type):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    overlay = img.copy()\n",
    "    \n",
    "    if exposure_type == \"Underexposed\":\n",
    "        mask = gray < 50\n",
    "        overlay[mask] = [0, 255, 255]\n",
    "    elif exposure_type == \"Overexposed\":\n",
    "        mask = gray > 205\n",
    "        overlay[mask] = [0, 165, 255]\n",
    "    \n",
    "    return cv2.addWeighted(img, 0.8, overlay, 0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a37228d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_exposure_path = \"IQA/detect_exposure\"\n",
    "for filename in os.listdir(detect_exposure_path):\n",
    "    img_path = os.path.join(detect_exposure_path, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "    height, width = img.shape[:2]\n",
    "    max_text_width = width - 20\n",
    "\n",
    "    exposure = classify_exposure(img)\n",
    "\n",
    "    cv2.putText(img, f\"{exposure}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "    \n",
    "    if exposure in [\"Underexposed\", \"Overexposed\"]:\n",
    "        img = highlight_exposure_regions(img, exposure)\n",
    "        \n",
    "        if exposure == \"Underexposed\":\n",
    "            enhancement_text = \"Enhancement: Increase brightness and adjust levels/curves to recover shadow detail.\"\n",
    "        else:  \n",
    "            enhancement_text = \"Enhancement: Decrease brightness and adjust levels/curves to recover highlight detail.\"\n",
    "        \n",
    "        draw_wrapped_text(img, enhancement_text, (10, 70), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, max_text_width)   \n",
    "    cv2.imshow(\"Image Quality Assessment - Exposure\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03ee0f",
   "metadata": {},
   "source": [
    "Detect Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8702291",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea16b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import is_low_contrast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b3a96",
   "metadata": {},
   "source": [
    "Highlight Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e34a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_low_contrast_regions(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    height, width = gray.shape\n",
    "    window_size = min(height, width) // 6\n",
    "    if window_size < 50:\n",
    "        window_size = 50\n",
    "    \n",
    "    overlay = img.copy()\n",
    "    \n",
    "    for y in range(0, height - window_size, window_size // 2):\n",
    "        for x in range(0, width - window_size, window_size // 2):\n",
    "            window = gray[y:y+window_size, x:x+window_size]\n",
    "            if is_low_contrast(window, 0.3):\n",
    "                cv2.rectangle(overlay, (x, y), (x+window_size, y+window_size), (255, 0, 0), -1)\n",
    "    \n",
    "    return cv2.addWeighted(img, 0.8, overlay, 0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf189382",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_contrast_path = \"IQA/detect_contrast\"\n",
    "for filename in os.listdir(detect_contrast_path):\n",
    "    img_path = os.path.join(detect_contrast_path, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "    height, width = img.shape[:2]\n",
    "    max_text_width = width - 20\n",
    "    \n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    text_contrast = \"Normal Contrast\"\n",
    "    enhancement_text_contrast = None\n",
    "\n",
    "    if is_low_contrast(gray, 0.35):\n",
    "        text_contrast = \"Low Contrast\"\n",
    "        enhancement_text_contrast = \"Enhancement: Increase contrast using curves, levels, or histogram equalization.\"\n",
    "        img = highlight_low_contrast_regions(img)\n",
    "    elif not is_low_contrast(gray, 0.85):\n",
    "        text_contrast = \"High Contrast\"\n",
    "        enhancement_text_contrast = \"Enhancement: Decrease contrast using curves or reducing highlights/shadows.\"\n",
    "\n",
    "    cv2.putText(img, f\"Contrast: {text_contrast}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "    if enhancement_text_contrast:\n",
    "        draw_wrapped_text(img, enhancement_text_contrast, (10, 70), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, max_text_width)\n",
    "\n",
    "    cv2.imshow(\"Image Quality Assessment - Contrast\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b0c2d",
   "metadata": {},
   "source": [
    "Detect Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52420cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_noise(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5,5), 0)\n",
    "    diff = cv2.absdiff(gray, blurred)\n",
    "\n",
    "    mean, std = np.mean(diff), np.std(diff)\n",
    "\n",
    "    if mean > 10 or std > 20:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa148852",
   "metadata": {},
   "source": [
    "Highlight Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5edfbd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_noisy_regions(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    height, width = gray.shape\n",
    "    window_size = min(height, width) // 8\n",
    "    if window_size < 50:\n",
    "        window_size = 50\n",
    "    \n",
    "    overlay = img.copy()\n",
    "    \n",
    "    for y in range(0, height - window_size, window_size // 2):\n",
    "        for x in range(0, width - window_size, window_size // 2):\n",
    "            window = gray[y:y+window_size, x:x+window_size]\n",
    "            blurred_window = cv2.GaussianBlur(window, (5, 5), 0)\n",
    "            diff = cv2.absdiff(window, blurred_window)\n",
    "            \n",
    "            mean_diff = np.mean(diff)\n",
    "            std_diff = np.std(diff)\n",
    "            \n",
    "            if mean_diff > 8 or std_diff > 15:\n",
    "                cv2.rectangle(overlay, (x, y), (x+window_size, y+window_size), (128, 0, 128), -1)\n",
    "    \n",
    "    return cv2.addWeighted(img, 0.8, overlay, 0.2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef89c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_noise_path = \"IQA/detect_noise\"\n",
    "for filename in os.listdir(detect_noise_path):\n",
    "    img_path = os.path.join(detect_noise_path, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "    height, width = img.shape[:2]\n",
    "    max_text_width = width - 20\n",
    "    \n",
    "    noise_detected = detect_noise(img)\n",
    "    text_noise = \"Not Noisy\"\n",
    "    enhancement_text_noise = None\n",
    "\n",
    "    if noise_detected:\n",
    "        text_noise = \"Noisy\"\n",
    "        enhancement_text_noise = \"Enhancement: Apply noise reduction filter (Gaussian, median, or bilateral filter).\"\n",
    "        img = highlight_noisy_regions(img)\n",
    "\n",
    "    cv2.putText(img, f\"Noise: {text_noise}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "    if enhancement_text_noise:\n",
    "        draw_wrapped_text(img, enhancement_text_noise, (10, 70), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2, max_text_width)\n",
    "    \n",
    "    cv2.imshow(\"Image Quality Assessment - Noise\", img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78f4ce2",
   "metadata": {},
   "source": [
    "# **Similarity Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8081af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = \"content/query.jpg\"\n",
    "landmark_dir = \"content/museum_images\"\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "landmark_descriptors = []\n",
    "landmark_keypoints = []\n",
    "landmark_images = []\n",
    "landmark_names = []\n",
    "\n",
    "for img_name in os.listdir(landmark_dir):\n",
    "    img_path = os.path.join(landmark_dir, img_name)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        continue\n",
    "    img = cv2.resize(img, (600, 400))\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    if des is not None:\n",
    "        landmark_keypoints.append(kp)\n",
    "        landmark_descriptors.append(des)\n",
    "        landmark_images.append(img)\n",
    "        landmark_names.append(img_name)\n",
    "\n",
    "def retrieve_top_similar_images(query_img, top_k=3, match_threshold=10):\n",
    "    query_img = cv2.resize(query_img, (600, 400))\n",
    "    query_kp, query_des = sift.detectAndCompute(query_img, None)\n",
    "    results = []\n",
    "\n",
    "    for i, des in enumerate(landmark_descriptors):\n",
    "        matches = bf.match(des, query_des)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "        good_matches = matches[:50]\n",
    "\n",
    "        if len(good_matches) >= match_threshold:\n",
    "            src_pts = np.float32([landmark_keypoints[i][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "            dst_pts = np.float32([query_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "            H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            if mask is not None:\n",
    "                inliers = int(np.sum(mask))\n",
    "                results.append((inliers, i, good_matches))\n",
    "\n",
    "    results.sort(reverse=True, key=lambda x: x[0])\n",
    "    top_images = []\n",
    "    for inliers, idx, good_matches in results[:top_k]:\n",
    "        matched_img = cv2.drawMatches(\n",
    "            landmark_images[idx], landmark_keypoints[idx],\n",
    "            query_img, query_kp, good_matches, None,\n",
    "            matchColor=(0, 255, 0), flags=2\n",
    "        )\n",
    "        top_images.append((landmark_names[idx], matched_img, inliers))\n",
    "    return top_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba30f620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match 1: Museum of Future 6.jpg with 32 inliers\n",
      "Match 2: Museum of Future 5.jpg with 8 inliers\n",
      "Match 3: Museum of Future 1.jpeg with 7 inliers\n"
     ]
    }
   ],
   "source": [
    "query_image = cv2.imread(query_path, cv2.IMREAD_GRAYSCALE)\n",
    "top_matches = retrieve_top_similar_images(query_image)\n",
    "\n",
    "for i, (name, img, inliers) in enumerate(top_matches):\n",
    "    print(f\"Match {i+1}: {name} with {inliers} inliers\")\n",
    "    cv2.imshow(f\"Top Match {i+1}\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0092f9",
   "metadata": {},
   "source": [
    "# **Annotation Capability**\n",
    "\n",
    "Enhanced as per Ms Asma's suggestions during the presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfea73",
   "metadata": {},
   "source": [
    "Drawing Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0765c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(event, x, y, flags, param):\n",
    "    global drawing, start_x, start_y\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        start_x, start_y = x, y\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            img_copy = img.copy()\n",
    "            cv2.rectangle(img_copy, (start_x, start_y), (x, y), (0, 255, 0), 2)\n",
    "            cv2.imshow(\"image\", img_copy)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        cv2.rectangle(img, (start_x, start_y), (x, y), (0, 255, 0), 2)\n",
    "        cv2.imshow(\"image\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad2b96",
   "metadata": {},
   "source": [
    "Drawing Circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fa9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(event, x, y, flags, param):\n",
    "    global drawing, start_x, start_y\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        start_x, start_y = x, y\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            img_copy = img.copy()\n",
    "            center_x = (start_x + x) // 2\n",
    "            center_y = (start_y + y) // 2\n",
    "            radius = int(0.5 * np.sqrt((x - start_x)**2 + (y - start_y)**2))\n",
    "            cv2.circle(img_copy, (center_x, center_y), radius, (0, 0, 255), 2)\n",
    "            cv2.imshow(\"image\", img_copy)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        center_x = (start_x + x) // 2\n",
    "        center_y = (start_y + y) // 2\n",
    "        radius = int(0.5 * np.sqrt((x - start_x)**2 + (y - start_y)**2))\n",
    "        cv2.circle(img, (center_x, center_y), radius, (0, 0, 255), 2)\n",
    "        cv2.imshow(\"image\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894fecd6",
   "metadata": {},
   "source": [
    "Drawing Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98988a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_line(event, x, y, flags, parmas):\n",
    "    global drawing, start_x, start_y\n",
    "\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "        start_x, start_y = x, y\n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing:\n",
    "            img_copy = img.copy()\n",
    "            cv2.line(img_copy, (start_x, start_y), (x, y), (255, 0, 0), 2)\n",
    "            cv2.imshow(\"image\", img_copy)\n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        cv2.line(img, (start_x, start_y), (x, y), (255, 0, 0), 2)\n",
    "        cv2.imshow(\"image\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33920466",
   "metadata": {},
   "source": [
    "Writing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8d5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(event, x, y, flags, param):\n",
    "    global img\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        text = input(\"Enter text to write: \")\n",
    "        cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"image\", img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74f5690",
   "metadata": {},
   "source": [
    "Main Annotation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d580e887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press keys to switch modes:\n",
      "1 - Circle mode (Red)\n",
      "2 - Rectangle mode (Green)\n",
      "3 - Line mode (Blue)\n",
      "4 - Text mode (Green) - Check console for input\n",
      "ESC - Exit\n",
      "s - Save image\n",
      "r - Reset image\n",
      "\n",
      "\n",
      "Switched to Rectangle mode (Green)\n",
      "Switched to Line mode (Blue)\n",
      "Switched to Text mode (Green)\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "drawing = False\n",
    "start_x, start_y = -1, -1\n",
    "current_mode = 1  # 1: Circle, 2: Rectangle, 3: Line, 4: Text\n",
    "\n",
    "\n",
    "def mouse_callback(event, x, y, flags, param):\n",
    "    global drawing, start_x, start_y, current_mode, img\n",
    "    \n",
    "    if current_mode == 1:  # Circle\n",
    "        draw_circle(event, x, y, flags, param)\n",
    "    elif current_mode == 2:  # Rectangle\n",
    "        draw_rectangle(event, x, y, flags, param)\n",
    "    elif current_mode == 3:  # Line\n",
    "        draw_line(event, x, y, flags, param)\n",
    "    elif current_mode == 4:  # Text\n",
    "        write_text(event, x, y, flags, param)\n",
    "\n",
    "\n",
    "img = cv2.imread(\"content/starry_night.jpg\")\n",
    "\n",
    "original_img = img.copy()\n",
    "\n",
    "cv2.namedWindow(\"image\")\n",
    "cv2.setMouseCallback(\"image\", mouse_callback)\n",
    "\n",
    "print(\"Press keys to switch modes:\")\n",
    "print(\"1 - Circle mode (Red)\")\n",
    "print(\"2 - Rectangle mode (Green)\")  \n",
    "print(\"3 - Line mode (Blue)\")\n",
    "print(\"4 - Text mode (Green) - Check console for input\")\n",
    "print(\"ESC - Exit\")\n",
    "print(\"s - Save image\")\n",
    "print(\"r - Reset image\")\n",
    "print('\\n')\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(\"image\", img)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == 27:\n",
    "        break\n",
    "    elif key == ord('1'):\n",
    "        current_mode = 1\n",
    "        print(\"Switched to Circle mode (Red)\")\n",
    "    elif key == ord('2'):\n",
    "        current_mode = 2\n",
    "        print(\"Switched to Rectangle mode (Green)\")\n",
    "    elif key == ord('3'):\n",
    "        current_mode = 3\n",
    "        print(\"Switched to Line mode (Blue)\")\n",
    "    elif key == ord('4'):\n",
    "        current_mode = 4\n",
    "        print(\"Switched to Text mode (Green)\")\n",
    "    elif key == ord('s'):\n",
    "        cv2.imwrite(\"drawn_image.jpg\", img)\n",
    "        print(\"Image saved as 'drawn_image.jpg'\")\n",
    "    elif key == ord('r'):\n",
    "        img = original_img.copy()\n",
    "        cv2.imshow(\"image\", img)\n",
    "        print(\"Image reset to original\")\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
